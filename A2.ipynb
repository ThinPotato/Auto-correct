{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 181,
     "status": "ok",
     "timestamp": 1615908962928,
     "user": {
      "displayName": "Yan Ma",
      "photoUrl": "",
      "userId": "13133035010264584735"
     },
     "user_tz": 240
    },
    "id": "keicKV6X8Y0Y"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import csv\n",
    "import math\n",
    "import pickle\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zk2tzdcynYn"
   },
   "source": [
    "# **Read Inputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lpfFShdy3e5J"
   },
   "outputs": [],
   "source": [
    "# Read user data\n",
    "lastWord = \"\"\n",
    "words = defaultdict(dict)\n",
    "with open('Data/Example/example_data.txt', 'r') as input:\n",
    "    for line in input:\n",
    "        line.strip\n",
    "        if line.startswith(\"===\"):\n",
    "            words[str(line)[3:len(str(line))-4]] = []\n",
    "            lastWord = str(line)[3:len(str(line))-4]\n",
    "        else:\n",
    "            touchpoints = str(line)[0:len(str(line))-2].split(\" \")\n",
    "            touchpoints[0] = str(line[0:1])\n",
    "            touchpoints[1] = float(touchpoints[1])\n",
    "            touchpoints[2] = float(touchpoints[2])\n",
    "            words[lastWord].append(touchpoints)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 857
    },
    "executionInfo": {
     "elapsed": 866,
     "status": "ok",
     "timestamp": 1615913307462,
     "user": {
      "displayName": "Yan Ma",
      "photoUrl": "",
      "userId": "13133035010264584735"
     },
     "user_tz": 240
    },
    "id": "34cXvsWno6Yv",
    "outputId": "96c3f6f7-2867-4996-bd8e-0190d649e806"
   },
   "outputs": [],
   "source": [
    "# Read dictionary\n",
    "with open('Data/unigram.dict', 'rb') as unigramModelFile:\n",
    "    unigramModel = pickle.load(unigramModelFile)\n",
    "unigramModelFile.close()\n",
    "print(unigramModel)\n",
    "\n",
    "# Read keyboard data\n",
    "keyboard_raw = pd.read_csv(\"Data/keyboard.csv\")\n",
    "keyboard = keyboard_raw[['key', 'x_mm', 'y_mm']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuntaNwhyuJj"
   },
   "source": [
    "# **Unigram Language Model Decoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 229,
     "status": "ok",
     "timestamp": 1615909214779,
     "user": {
      "displayName": "Yan Ma",
      "photoUrl": "",
      "userId": "13133035010264584735"
     },
     "user_tz": 240
    },
    "id": "AaTGhNueN9hD"
   },
   "outputs": [],
   "source": [
    "# Keyboard size and dual Gaussian model parameters\n",
    "key_width = 3\n",
    "key_height = 4\n",
    "a = 2.403\n",
    "b = 0.017\n",
    "c = 2.295\n",
    "d = 0.016\n",
    "\n",
    "def get_likelihood(p, mu, sigma):\n",
    "    \"\"\"\n",
    "    TODO: Calculate the likelihood that a touch point p is from the 2D Gaussian distribution N(mu, sigma)\n",
    "    \"\"\"\n",
    "\n",
    "    lik = 1/((math.pow(2*math.pi*math.pow(sigma,2)), 1/2)) * np.exp(-(1/(2*math.pow(sigma,2)))(math.pow(p-mu,2)))\n",
    "    return lik\n",
    "\n",
    "def is_letter(p, letter):\n",
    "    \"\"\"\n",
    "    TODO: Determine if touch point p is located inside the boundary of the key: letter\n",
    "    \"\"\"\n",
    "    keyWidth = 2.9750073\n",
    "    keyHeight = 4.02501\n",
    "    for key in keyboard:\n",
    "        if key[1] == letter:\n",
    "            if p[0] <= key[2] + keyWidth and p[0] >= key[2] and p[1] <= key[3] + keyHeight and p[1] >= key[3]:\n",
    "                return true\n",
    "    return False \n",
    "\n",
    "def get_literal_string(touchpoints):\n",
    "    \"\"\" \n",
    "    TODO: Compute the literal string using is_letter(p, letter) method for a collection of touch points that represents a word. \n",
    "          If a touch point does not fall inside any key boundary, use '?' to represent the corresponding character.\n",
    "    \"\"\"\n",
    "    literal_string = \"\"\n",
    "    for touchpoint in touchpoints:\n",
    "        found = false\n",
    "        for letter in keyboard:\n",
    "            if is_letter(touchpoint, letter[1]):\n",
    "                literal_string += letter[1]\n",
    "                found = true\n",
    "        if found == false:\n",
    "            literal_string+= \"?\"\n",
    "\n",
    "    return literal_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-5b1b11244d20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtouchpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfirstTP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtouchpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlastTP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtouchpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtouchpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpossible_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munigramModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "possible_words = []\n",
    "length = len(touchpoints)\n",
    "firstTP = touchpoints[0]\n",
    "lastTP = touchpoints[len(touchpoints)-1]\n",
    "possible_words = dict(filter(lambda x: len(str(x[0])) == length, unigramModel.items()))\n",
    "print(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1615909358093,
     "user": {
      "displayName": "Yan Ma",
      "photoUrl": "",
      "userId": "13133035010264584735"
     },
     "user_tz": 240
    },
    "id": "_op29UXGRmc0"
   },
   "outputs": [],
   "source": [
    "def unigram_lm_decoder(touchpoints):\n",
    "    \"\"\"\n",
    "    A language decoder that uses the dual Gaussian touch point spatial disrtibution model and a unigram language model.\n",
    "    Input: a list/collection of touch points that represents a certain word\n",
    "    Output: the decoded word for the input\n",
    "    \n",
    "    TODO: Step a --- Get all possible words and their corresponding probabilities from the dictionary. \n",
    "          Use the length of the correct word to filter possible words\n",
    "          You may also use the first and/or the last touchpoint to further narrow down possible words\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\" sum =0\n",
    "    for k in range(p):\n",
    "        x = -(1)/(2* math.pow(sigma,2))*math.pow(p[k]-mu,2)\n",
    "        sum += (math.pow(x, k))/(math.factorial(k))\n",
    "\n",
    "    lik = 1/((math.pow(2*math.pi*math.pow(sigma,2)), 1/2)) * sum\n",
    "    \"\"\"\n",
    "    possible_words = []\n",
    "    length = len(touchpoints)\n",
    "    firstTP = touchpoints[0]\n",
    "    lastTP = touchpoints[len(touchpoints)-1]\n",
    "    possible_words = dict(filter(lambda x: len(str(x[0])) == length, unigramModel.items()))\n",
    "\n",
    "    \n",
    "    # Calculate p(w|s_1, s_2, ... s_n) ~ p(s_1, s_2, ..., s_n|w)*p(w) = \\Pi(p(s_i|c_i))p(w) for each possible word\n",
    "    p_w_s = []                  # Holds p(w|s_1, s_2, ..., s_n) for all possible words\n",
    "    for item in possible_words:\n",
    "        word =                    # TODO: The current possible word\n",
    "        p_w =                     # TODO: Probability of the current possible word in the unigram language model\n",
    "        p_s_w = 1                 # Holds p(s_1, s_2, ..., s_n|w) for the current possible word\n",
    "\n",
    "        for j, letter in enumerate(list(word)):\n",
    "            # TODO: Step b --- Apply the spatial model to get p(s_i|c_i)\n",
    "            mu =\n",
    "            sigma =\n",
    "            p_s_c = \n",
    "            \n",
    "            # TODO: Step b --- Multiply the current p(s_i|c_i) to p(s_1, s_2, ..., s_n|W)\n",
    "        \n",
    "        # TODO: Step c --- Calculate p(w|s_1, s_2, ... s_n) from p(s_1, s_2, ..., s_n|w) and p(w). Append the result to list\n",
    "    \n",
    "    # TODO: Step d ---- Choose word by the maximum of p(w|s_1, s_2, ..., s_n)\n",
    "    decoded_word = \"\"\n",
    "    \n",
    "    return decoded_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WrRbMoZRCZ_u"
   },
   "outputs": [],
   "source": [
    "decoded_success_count = 0\n",
    "literal_success_count = 0\n",
    "decoded_words = []\n",
    "literal_strings = []\n",
    "correct_words = []\n",
    "for touchpoints in words:\n",
    "    \"\"\"\n",
    "    TODO: Use above methods to compute the correct word, decoded word, and the literal string for each touch point collection\n",
    "          Append results to the corresponding list\n",
    "          Update the decoded words/literal strings success count. \n",
    "              --- If the decoded word/literal string is the same as correct word, increase 1 to decoded words/literal strings success count\n",
    "    \"\"\"\n",
    "    \n",
    "# TODO: calculate the success rate for both the decoded words and the literal strings using the docoded word/literal string success count\n",
    "\n",
    "# TODO: Write to results.txt\n",
    "with open(\"results.txt\", 'w') as output:\n",
    "    # The first line: success_rate(decoded_words), success_rate(literal_strings)\n",
    "    for i in range(len(correct_words)):\n",
    "        # Each line after: correct_word, decoded_word, literal_string\n",
    "\n",
    "output.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "A2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "name": "python392jvsc74a57bd0b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "metadata": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}